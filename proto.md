# What are fibered type families good for?

Finitary type families abstractly embody formalized languages.
For example, consider the following simple language of arithmetic
and logical expressions:
```
data ExpressionKind
  Numeric
  Logical

data Expr : ↓ExpressionKind
  Literal(n : Int)          : Expr(Numeric)
  Sum(a b : Expr(Numeric))  : Expr(Numeric)
  Mul(a b : Expr(Numeric))  : Expr(Numeric)
  Div(a b : Expr(Numeric))  : Expr(Numeric)
  Pow(a b : Expr(Numeric))  : Expr(Numeric)
  Neg(a : Expr(Numeric))    : Expr(Numeric)
  Log(a : Expr(Numeric))    : Expr(Numeric)
  
  Eq(a b : Expr(Numeric))   : Expr(Logical)
  Less(a b : Expr(Numeric)) : Expr(Logical)
  And(a b : Expr(Logical))  : Expr(Logical)
  Or(a b : Expr(Logical))   : Expr(Logical)
  Not(a b : Expr(Logical))  : Expr(Logical)
```

This approach scales up to languages that may declare and bind named
entities (variables, constants, internal types) including general-purpose
programming languages themselves.

Data types defined that way are inhabited by abstract syntax trees
corresponding to finite expressions of the language, and they come
with a recursive descent analysis operator that enabling
type-driven design of correct-by-construction analysers and interpreters.
This includes type checking, compilation, control flow analysis,
as well as static analysis and abstract interpretation in general.

As for IDEs, inductive type families enable designing biparsers for
those languages, i.e. parsers that maintain one-to-one mapping
between the source code and the respective annotated abstract syntax
tree, enabling both fast incremental reparsing and mechanized refactoring.

* * *

To represent languages with typed variables, one introduces the type `Ty`
representing variable types in the language, and the type family `Tm : ↓Ctx`
of terms in a given context, where contexts are lists of types `Ctx := Ty*`.
Definition of term substitution can be vastly simplified if we make the type
`Ctx` of contexts fibered over the shape type `LaxNat` that enables context
extension and selection of subcontexts.

In case of dependently typed languages, the we’ll have a type family `Ty : ↓Ctx`
of variable types available in a given context `c : Ctx`, and the type of contexts
is an iterated dependent pair type
```
data Ctx
  Empty : Ctx
  Append(prefix : Ctx, head : Ty prefix)
```
fibered over the shape type Δ that enables context extension and
selection of subcontexts and respecting type dependencies, which is
a absolutely vital for the definition of the aforementioned type family
`Ty : ↓Ctx` and the type family `Tm : ↓(c : Ctx, Ty c)` of terms of a
given type in a given context.

Bi-directionally typed languages also
require a fibered type family `Redex / type : ↓(c : Ctx) ↑(Ty c) ` of
reducible expressions synthesizing their types.

Now as we have motivated the need for all this stuff, let's dive in.

# Type families and inverse categories

For a type `J : 𝒰` let `↓J` denote the respective universe of type families indexed by `J`. 
A typical example are length-indexed lists:
```
data Vec<T> : ↓Nat
  nil : Vec<T> 0
  cons<n>(head : T, tail : Vec<T> n) : Vec<T> n⁺
```

Ordinary inductive types are freely generated by their generators.
Quotient inductive types may additionally contain constructors of relations (i.e. identities) between inhabitants.

This way we can define rational numbers and unordered collections:
```
data Rational
  frac(num : Int, den : PosInt)
  expand<num, den>(n : PosInt) : frac(num, den) = frac(num · n, den · n)
  
data Collection<T>
  bagOf<n : FinType>(items : n → T)
  permute<n, items>(p : n!) : bagOf(items) = bagOf(items ∘ p)
```
where `n!` is the type of automorphisms on the type `n`, i.e. permutations in case of finite types.

That is, in addition to listing generators, we require that some actions
on generators (expanding the fraction or permuting list elements) must
be irrelevant for all predicates and functions defined on these types.

A more interesting example is given by the type of eventually-zero sequences:
```
data ZeroEndingSequence
  nil : ZeroEndingSequence
  append(prefix : ZeroEndingSequence, head : Nat)
  extend(f : ZeroEndingSequence) : f = append(f, 0)
```

As we have seen above, we can turn the type of lists to a length-indexed type family over `Nat`,
but we cannot make `ZeroEndingSequence` into a type family over `Nat` because
`extend` generates equality between “lists” of different lengths. We need
a “lax” index type instead of `Nat`:
```
shape LaxNat
  lax(n : Nat) : LaxNat
  extend(n : Nat) : (l : LaxNat) → when(l)
    lax(m) ↦ ↑l lax(n + m)
    extend(m) ↦ extend(n + m)
```

While quotient inductive types admit constructors of identities between their elements,
inductive shape types admit constructors of extensions “between” their elements.
In synthetic types, for any two elements `x y : T` we have an identity type
`x = y : 𝒰`. In shape types, for every element `x : P` we have a `P`-indexed
type family `↑l : ↓P`. Its elements are the extenders from the element
`l`, and their indexes are the targets of extensions. Sources of extenders
must be structurally smaller than their targets to enable typechecking.

Every function we define on a shape type must have an action on all constructors,
including extension constructors. The action of an extension constructor on the
other extension constructors are their composition. The action of an
extension constructor on extension constructors must have the form 
of function application, i.e. `extend(m) ↦ extend(f m)` so the typechecker
can ensure that the composition is associative by construction.

This way, shape types form strictly associative inverse categories.

To have an example for other functions, let us define addition for
`LaxNat`s:
```
def add : LaxNat² → LaxNat
  (lax(n), lax(m)) ↦ lax(m + n)
  (extend(n) _, _) ↦ extend(n)
  (_, extend(n) _) ↦ extend(n) 
```

With `LaxNat` we can transform `ZeroEndingSequence` into a type family:
```
data ZeroEndingSizedSequence : LaxNat↓
  nil : ZeroEndingSizedSequence lax(0)
  append<n>(prefix : ZeroEndingSizedSequence n, head : Nat) : ZeroEndingSizedSequence (lax(1) + n) 
  extend<n>(f : ZeroEndingSizedSequence n) : ???
```

Before we fill in the gap in the above definition, note that type families also seem to be functions on their index type,
so they must act on the extension constructors.
However, the only proper action would be domain extension for functions defined
on those type families. Let `F : ↓I` be a type family, and `e : ↑n m` for some `n m : I`.
Then `F(e) : ∀<Y> (F(n) → Y) → (F(m) → Y)`. We also have a dependently typed version.
```
F(e) : ∀<Y : ↓F(n)> (∀(x : F(n)) Y(x)) → (∀(x : F(m)) F(e) Y)(x))
```

Now we can fill in the gap in the definition of `ZeroEndingSizedSequence`. The type
of the equality constructor `f = append(f, 0)` does not typecheck yet, but we can
decompose it into an application `{ it = append(f, 0) } f` and apply the domain
extension to the function part by applying `ZeroEndingSizedSequence (extend(1) n)`:
```
data ZeroEndingSizedSequence : ↓LaxNat
  nil : ZeroEndingSizedSequence lax(0)
  append<n>(prefix : ZeroEndingSizedSequence n, head : Nat) : ZeroEndingSizedSequence (lax(1) + n) 
  extend<n>(f : ZeroEndingSizedSequence n)
  : ZeroEndingSizedSequence (extend(1) n) { it = append(f, 0) } f
```

# Fibered types and direct categories

Many operations on containers have the following property:
the shape of the resulting container only depends on shapes of the arguments.
For example, length of the list computed by `concatenate`, `map`, and `reverse`
can be computed based on the lengths of the input lists.

Let us introduce a notion of fibered types. 
For every type `B : 𝒰` we'll have a type family `↑B` indexed by types `T : 𝒰`.
In contrast to ordinary type families, `↑B` act as a postfix operators on their index `T`:
```
inductive ↑B : ↓𝒰
  fiberedType(F : 𝒰, f : F → B) : F ↑B 
```

For shortness let us denote `fiberedType(F, f)` by `F / f`.

For example, we can take the type of lists `T*` and the function `length : T* → ℕ`:
`(T* / length) : T* ↑ℕ`.

As already mentioned in the previous section, for a type `J : 𝒰` we have the type `↓J`
of `J`-indexed type families.
The type former `Σ<J> : ↓J → 𝒰` makes it a fibered type: `↓J / Σ : ↓J ↑𝒰`.

For every type-valued function `Y : B → 𝒰`, we have the fibered type `ΣY / fst : ΣY ↑B`.
Owing to equality, we can invert this operation (for ordinary types, not shape types):
for every fibered type `F / f : F ↑B` we have a function
`{ b : B ↦ Σ(x : F) f(x) = b} : B → 𝒰`.

In case of fibered inductive types `F / f`,
the type `F` and the function `f` can be defined simultaneously in a mutually
dependent fashion, which is already supported in some dependently-typed languages
as inductive-recursive types.

A function between fibered types is a pair of functions `(f / b) : (X / p) → (Y / q)`,
so that the following square commutes by construction:
```
 X --[f]--> Y
 |p         |q
 V          V
 A --[b]--> B
```

(Functions `f` and `b` can be defined by induction simultaneously in a mutually
dependent fashion when necessary.)

Consider a few examples of functions on fibered types:
```
reverse<T> / id  : (T* / length)  → (T* / length)
concat<T> / add  : (T* / length)² → (T* / length)
flatten<T> / sum : (T* / length)* → (T* / length)

map<X, Y>(f : X → Y) / id : (X* / length)  → (Y* / length)
```

Fibered types have non-trivial behaviour with respect to type families indexed
over them.
For a fibered type `F / f : F ↑B` and a type-family `Y : ↓(F / f)` indexed over
it, and an element `x : F` we have the following rule:
```
Y(x) : Bᵈ (f x) Y
```
where `Bᵈ` is displayed counterpart of `B` as introduced in [[dTT]] paper.

Inductive types can be self-fibered:
```
shape D / select : * ↑D
  fst : D
  snd : D
  def select : D → * ↑D
    fst ↦ (Void / { it })
    snd ↦ (Unit / { fst })
```

Inductive self-fibered types form strictly associative direct categories. (TODO: Clarify)

A type family `Y : ↓(D / select)` indexed over this type satisfies the following typing rule:
```
Y(x : D) : (* ↑D)ᵈ (select x) Y
```
 
Since `D` only has two elements, we can split cases:
```
Y(fst) : (* ↑D)ᵈ (select fst) Y
Y(snd) : (* ↑D)ᵈ (select snd) Y
```
which in turn reduces to
```
Y(fst) : (* ↑D)ᵈ (Void / { it }) Y
Y(snd) : (* ↑D)ᵈ (Unit / { fst }) Y
```
further reducing to
```
Y(fst) : (∀(u / f : (Void / { it })) Y(f(u))) → 𝒰
Y(snd) : (∀(u / f : (Unit / { fst })) Y(f u)) → 𝒰
```

Product over empty domain is `Unit`, and the product over unit domain is just one element:
```
Y(fst) : Unit → 𝒰
Y(snd) : Y(fst) → 𝒰
```
So our type family is merely a dependent pair `Σ(T : 𝒰) (T → 𝒰)`!

With self-fibered index types we can define dependent pairs as dependent function types.
Signatures of theories with dependent sorts can be expressed as finite direct categories,
so first-order and higher-order theories with dependent sorts can be expressed as type
classes parametrized by such families.
Algebraic theories with dependent sorts can be
expressed via inductive type families indexed over a finite self-fibered index type S.
In particular categories are models of an algebraic theory over the shape
```
shape CellType / select : * ↑CellType
  Ob : CellType
  Mor : CellType
  def select : CellType → * ↑CellType
    Ob ↦ (Void / { it })
    Mor ↦ (Bool / { Ob })
```

To deal with ∞-categories, one can introduce a shape types `CellType` containing cell types
of every dimension `n : Nat`.

Dually to our lax natural numbers, we can introduce a self-indexed type `Δ⁺`:
```
shape Δ⁺ / select : * ↑Δ⁺
  simplex(n : Nat) : Δ⁺
  def select : Δ⁺ → * ↑Δ⁺
    simplex(n) ↦ (Σ(m) Fin(m) → Fin(n)) / simplex(m)
```

Type families over Δ⁺ are semi-simplicial types.
Type families over thin (i.e. with at most one arrow between any two inhabitants)
self-indexed types are also known as very dependent types.

Most notably, we can combine extensions (degeneracy maps) and selections (face maps)
yielding strictly associative Reedy categories like the simplicial category Δ:
```
shape Δ / select : * ↑Δ
  simplex(n : Nat) : Δ
  extend : (s : Δ) → when(s)
    simplex(n) ↦ (Σ(m) Fin(m) → Fin(n)) → ↑s simplex(m)
    ... simplicial identities part I
  def select : Δ⁺ → * ↑Δ⁺
    simplex(n) ↦ (Σ(m) Fin(m) → Fin(n)) / simplex(m)
    ... simplicial identities part II
```

Type families `F : ↓Δ` on Δ are the simplicial types.

As we already mentioned above, the shape type Δ is vital for defining the syntax of dependent typed theories.

Notably, universes of types, type families, and fibered types/type families also carry a shape structure
with selections given by fibered types and extensions given by type families and a compatible proarrow
equipment given by display operator ( ᵈ).
Universes of models for any given algebraic theory also carry a shape structure and a compatible proarrow
equipment.

# What are functorial type families good for?

Finitary type families abstractly embody formalized languages.
For example, consider the following simple language of arithmetic
and logical expressions:
```
data ExpressionKind
  Numeric
  Logical

data Expr : ↓ExpressionKind
  Literal(n : Int)          : Expr(Numeric)
  Sum(a b : Expr(Numeric))  : Expr(Numeric)
  Mul(a b : Expr(Numeric))  : Expr(Numeric)
  Div(a b : Expr(Numeric))  : Expr(Numeric)
  Pow(a b : Expr(Numeric))  : Expr(Numeric)
  Neg(a : Expr(Numeric))    : Expr(Numeric)
  Log(a : Expr(Numeric))    : Expr(Numeric)
  
  Eq(a b : Expr(Numeric))   : Expr(Logical)
  Less(a b : Expr(Numeric)) : Expr(Logical)
  And(a b : Expr(Logical))  : Expr(Logical)
  Or(a b : Expr(Logical))   : Expr(Logical)
  Not(a b : Expr(Logical))  : Expr(Logical)
```

This approach scales up to languages that may declare and bind named
entities (variables, constants, internal types) including general-purpose
programming languages themselves.

Data types defined that way are inhabited by abstract syntax trees
corresponding to finite expressions of the language, and they come 
with a recursive descent analysis operator that enabling
type-driven design of correct-by-construction analysers and interpreters.
This includes type checking, compilation, control flow analysis,
as well as static analysis and abstract interpretation in general.

As for IDEs, inductive type families enable designing biparsers for
those languages, i.e. parsers that maintain one-to-one mapping
between the source code and the respective annotated abstract syntax
tree, enabling both fast incremental reparsing and mechanized refactoring.

To represent syntax of dependently typed languages, we’ll have to introduce
the inductive type family `Ty : ↓Ctx` of language types definable in the given
context `c : Ctx`, where the type of contexts is an iterated dependent pair type
```
data Ctx
  Empty : Ctx
  Append(prefix : Ctx, head : Ty prefix)
```
fibered over a special prototype Δ that enables context extension and 
selection of subcontexts and respecting type dependencies, which is
a prerequisite for the definition of the inductive type families
`Ty : ↓Ctx` of types in a given context and `Tm : ↓(c : Ctx, Ty c)`
of terms of a given type in a given context, as well as a fibered
type family `Redex / type : ↓(c : Ctx) ↑(Ty c) ` of reducible expressions
synthesizing their types.


## Sum types

Let us start from finite datatypes (also known as enums) defined by enumerating their possible values:
```
synthetic Boolean
  True
  False
```

We can generalize them to so-called sum types by allowing infinite families of “possible values”
parametrized by some other type:
```
synthetic Possibly<X>
  Nothing
  Value(x : X)
```

Each synthetic type comes along with a dual “case analyser” type:
```
structure Booleanᴿ<Y>
  true  : Y
  false : Y
```
```
structure Possiblyᴿ<X, Y>
  nothing : Y
  value(x : X) : Y
```

Inhabitants of synthetic types `x : T` can be converted into functions
`xᶜ : ∀<Y> Tᴿ<Y> → Y`, which is known as Church encoding:
```
def <Y> Trueᶜ(m : Booleanᴿ<Y>)  = m.true
def <Y> Falseᶜ(m : Booleanᴿ<Y>) = m.false

def <X, Y> Nothingᶜ(m : Possiblyᴿ<X, Y>)  = m.nothing
def <X, Y> Value(x : X)ᶜ(m : Possiblyᴿ<X, Y>)  = m.value(x)
```

What if we want to return values of different types for `True` and `False`?
If we have universes (types of types), we can first define a function from
booleans into some universe `R : Boolean → 𝒰` and then a dependent case analyser
```
structure Booleanᴹ<Y : Boolean → *>
  true  : Y(True)
  false : Y(False)
```

To apply dependent case analysers to inhabitants of the respective type we
need special operator called induction for reasons explained below:
```
I-ind : ∀<Y> Iᴹ<Y> → ∀(x : I) Y(x)
```

# Inductive types

The next step is to allow well-founded recursion in type definitions.
In this way we can introduce natural numbers, lists, and trees:
```
synthetic Nat `ℕ`
  Zero `0`
  Next(pred : ℕ) `pred⁺`
```
```
synthetic List<T>
  EmptyList : List<T>
  NonEmptyList(head : T, tail : List<T>) : List<T>
```
```
synthetic BinTree<T>
  Leaf
  Node(label: T, left : BinTree<T>, right : BinTree<T>)
```
```
synthetic VarTree<T>
  Leaf
  Node(label: T, branches : List<VarTree<T>>)
```
```
synthetic InfTree<T>
  Leaf
  Node(label: T, branches : Nat → InfTree<T>)
```

All above examples except infinitely branching trees are closed inductive types:
all of their generators have a finite number of parameters of closed inductive types.
Close inductive types embody single-sorted languages. They are inhabited by
abstract syntax trees corresponding to finite expressions of the language
formed by their generators.

“Case analysis” for the type of natural numbers provides n-ary iteration operator:
```
structure Natᴿ<Y>
  zero : Y
  next(p : Y) : Y
```
Analysing a natural number `n` by `m : Natᴿ<Y>` yields `cᶜ(m) = (m.next)ⁿ b.zero`,
allowing to iterate arbitrary functions given number of times. In general,
“case analysis” turns into “recursive descent analysis”. For lists and trees we
obtain the respective fold operators.

Type-valued functions on natural numbers `Y : Nat → 𝒰` can encode arbitrary predicates,
and a dependent `Nat`-analyser `Natᴹ<Y>` encodes an induction motive: it establishes
a proof of the base case `Y(zero)` and the inductive step `Y(n) → Y(n⁺)`.
Dependent case analysis operator turns induction motives into to proof the predicate
for all natural numbers, that is why it is also known as induction operator.
The presence of induction witnesses that inductive types contain only inhabitants that
can be obtained by finite compositions of their generators. Which is also the reason
why data types described in terms of their generators are called inductive types.

# Synthetic types

Inductive types are freely generated by their generators. Synthetic types (also known
as quotient inductive types) are described in terms of generators and relations.

This way we can define rational numbers and unordered collections:
```
synthetic Rational `QQ`
  frac(num : Int, den : PosInt)
  expand<num, den>(n : PosInt) : frac(num, den) = frac(num · n, den · n)
  
synthetic Collection<T>
  bagOf<n : FinType>(elements : n → T)
  permute<n, elements>(p : n!) : bagOf(elements) = bagOf(elements ∘ p)
```
Let `n!` be the type of permutations 

That is, in addition to listing generators, we require that some actions
on generators (expanding the fraction or permuting list elements) must
be irrelevant for all predicates and functions defined on these types.

# Algebraic theories

`Iᴿ<T>` equips `T` with a structure of an I-algebra. These structures form
a displayed category, with an initial object given by `I` and its generators.

`Iᵈ<T>` is the free I-algebra on `T`, an initial object among I-algebra
structures on `T`. The container `Iᵈ` is thus automatically a monad.

A monadic map from `Iᵈ<T>` to `T` is the same as the structure `Iᴿ<T>`.

!!! And it is the same as an Quillen equivalence from ??? to `T`.

```
(l : List<T>) ~> Vec<T>(l.length)

T -[just]-> Expr<T>
  <-[eval]- 
  
Any `eval` that commutes with `just` is monadic?

Expr<T> -
```

# Type families

But what about languages that contain multiple sorts of expressions
and the ones where expression can contain variables?



In this case, instead of a single closed inductive type Expr we have a type family
`Expr : Ctx -> *`, that describes expression within a given context.
In the simplest case, the context is just a natural number — the number of variables.
The type `Expr(0)` denotes closed expressions, `Expr(1)` expressions with one variable,
`Expr(2)` expressions with two variables, and so on. In fact, `Ctx` is not just the type
`Nat`, but carries additional information. For every `ctx : Ctx` we have (subcontext)
selectors `h : ctx↓` and context extenders `h : ctx↑` mapping `ctx` either to a 
smaller or a larder context `|h|` respectively.

Let us call types augmented by selectors and extenders prototypes. One of the simplest
prototypes is `Δ⁺`, which is a glorified type of natural numbers, where selectors
`p : n↓` are the ways to select `m < n` elements among `n` and extenders `e : n↑` are
the ways to put additional variables before, between, and after existing ones.
Selectors and extenders have their target shapes denoted by `|x|`.

Extenders of prototypes carry over to type families indexed over them.
In particular, extenders in `Ctx` carry over to expressions.
That is, every expression `expr : Expr(ctx)` extends to an expression in the larger context:
For `e : ctx↑`, we have `(e expr) : Expr(e ctx)`.

In general, contexts may contain more information than the number of variables.
For instance, their types. In this case, the type `Ctx` is not `Δ⁺` itself, but
fibered over `Δ⁺` by the projection `length : Ctx => Δ⁺`.

Selectors and extenders in `S` carry over to the types fibered over `S`. That is,
given a type of contexts `ctx : Ctx(length = n)`, we can apply a selector `p : n↓`
obtaining the type of subcontexts `(p ctx) : Ctx(length = |p|)`.
All selectors in `Ctx` must come from selectors in `Δ⁺`.
The type of contexts itself may also describe extenders that add additional
variables of given types to the context.
All extenders in `Ctx` must have a preimage in `Δ⁺`.

Selectors 

* * *

??? What do selectors do?

* * *

Algebraic theories = self-fibered prototypes

Unbiased monoid prototype?

Define “we have tensor product on `Ob`” by
“we have a monadic function eval : `List<Ob> -> Ob`”, where monadicity means
```
∀(l : List<List<T>>) we have an extension from
  eval(l.map(eval))
to
  eval(l.flatten)
```

Assume we have a type of monoidal expressions `VariadicTree<T>`, and for every non-flat `t` it only extension
maps it to `t.flatten`. Then we have an action of these extensions on functions from `VariadicTree<T>`, rendering
functions monadic (does it?).

??? Can we obtain such VariadicTrees as algebras over the unbiased monoid prototype?


# Synthetic types

Postulate that the order of list elements must be irrelevant
Postulate that multiplying numerator and denominator by the same positive integer must be irrelevant



TODO: Clarify how `T↑` works for `T : 𝒰`.
Is it
```
inductive T↑ : 𝒰 → 𝒰
  fam(F : T → 𝒰) : T↑ (Σ(T) F) 
```

# Induced structure

For every inductive type `I` we can define a derived prototype of expressions `I'<T : *>`, e.g.
```
prototype Nat'<T>
  pure : T → Nat'<T>
  zero : Nat'<T>
  succ : Nat'<T> → Nat'<T>
```

Expressions in the context `C<T>` are elements of `∀<T> □(C<T> → Th'<T>)`.

So we have `pure<T> : T → Th'<T>` its adjunctions are maps `f : Th'<T> → T` so that
`∀(x : T) f(pure x) = x` and every preimage `x` is extendable from `pure(x)`.

Adjunctions to pure induce lax Th-structure onto the type T.

* * *

